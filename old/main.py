# rag_gradio_app.py

import os
import json
import pickle
import re
import time
from pathlib import Path
from typing import List, Dict, Tuple, Optional
from dataclasses import dataclass

import torch
import numpy as np
import faiss
import jieba
import gradio as gr
import fitz  # PyMuPDF
from sentence_transformers import SentenceTransformer
from openai import OpenAI
from transformers import AutoTokenizer, AutoModel

# -------------------- æ•°æ®ç»“æ„å®šä¹‰ --------------------

@dataclass
class Document:
    id: str
    title: str
    content: str
    source: str
    metadata: Dict
    embedding: Optional[np.ndarray] = None

@dataclass
class QueryResult:
    query: str
    answer: str
    source_documents: List[Document]
    confidence_score: float
    retrieval_time: float
    generation_time: float

# -------------------- æ–‡æ¡£å¤„ç†æ¨¡å— --------------------

class DocumentProcessor:
    def __init__(self, chunk_size=512, chunk_overlap=50):
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap

    def clean_text(self, text: str) -> str:
        text = re.sub(r'\s+', ' ', text)
        text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\s\.,!?;:""\'\(\)ï¼ˆï¼‰ã€ã€‘\-]', '', text)
        return text.strip()

    def split_text_by_sentences(self, text: str) -> List[str]:
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿï¼›\n]', text)
        return [s.strip() for s in sentences if s.strip()]

    def create_chunks(self, text: str) -> List[str]:
        sentences = self.split_text_by_sentences(text)
        chunks, current_chunk = [], ""
        for sentence in sentences:
            if len(current_chunk) + len(sentence) <= self.chunk_size:
                current_chunk += sentence + "ã€‚"
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = sentence + "ã€‚"
        if current_chunk:
            chunks.append(current_chunk.strip())
        return chunks

    def process_document(self, content: str, title: str, source: str, metadata: Dict = None) -> List[Document]:
        if metadata is None:
            metadata = {}
        clean_content = self.clean_text(content)
        chunks = self.create_chunks(clean_content)
        return [Document(
            id=f"{source}_{i}", title=title, content=chunk, source=source,
            metadata={**metadata, 'chunk_index': i, 'total_chunks': len(chunks)})
            for i, chunk in enumerate(chunks)
        ]

# -------------------- å‘é‡æ„å»ºæ¨¡å— --------------------

class EmbeddingModel:
    def __init__(self, model_name="BAAI/bge-large-zh-v1.5"):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model = SentenceTransformer(model_name, device=self.device)
        self.embedding_dim = self.model.get_sentence_embedding_dimension()

    def encode_texts(self, texts: List[str]) -> np.ndarray:
        return self.model.encode(texts, show_progress_bar=False, convert_to_numpy=True)

    def encode_query(self, query: str) -> np.ndarray:
        return self.encode_texts([query])[0]

# -------------------- å‘é‡åº“ç®¡ç†æ¨¡å— --------------------

class VectorStore:
    def __init__(self, embedding_dim: int, index_path="vector.index", docs_path="documents.pkl"):
        self.embedding_dim = embedding_dim
        self.index_path = index_path
        self.docs_path = docs_path
        
        if os.path.exists(self.index_path) and os.path.exists(self.docs_path):
            self.index = faiss.read_index(self.index_path)
            with open(self.docs_path, "rb") as f:
                self.documents = pickle.load(f)
        else:
            self.index = faiss.IndexFlatIP(embedding_dim)
            self.documents = []

    def save(self):
        faiss.write_index(self.index, self.index_path)
        with open(self.docs_path, "wb") as f:
            pickle.dump(self.documents, f)

    def add_documents(self, docs: List[Document], embeddings: np.ndarray):
        faiss.normalize_L2(embeddings)
        self.index.add(embeddings)
        for i, doc in enumerate(docs):
            doc.embedding = embeddings[i]
        self.documents.extend(docs)
        self.save()

    def search(self, query_embedding: np.ndarray, top_k=5) -> List[Tuple[Document, float]]:
        if len(self.documents) == 0:
            return []  # å‘é‡åº“ä¸ºç©ºç›´æ¥è¿”å›ç©ºåˆ—è¡¨

        query_embedding = query_embedding.reshape(1, -1)
        faiss.normalize_L2(query_embedding)
        scores, indices = self.index.search(query_embedding, top_k)

        return [
            (self.documents[idx], float(scores[0][i]))
            for i, idx in enumerate(indices[0])
            if 0 <= idx < len(self.documents)
        ]

    def list_documents(self, max_chars=100) -> List[str]:
        return [f"[{i}] {doc.title} - {doc.content[:max_chars]}..." for i, doc in enumerate(self.documents)]

    def search_by_keyword(self, keyword: str, max_results=10) -> List[str]:
        results = []
        for i, doc in enumerate(self.documents):
            if keyword in doc.content or keyword in doc.title:
                results.append(f"[{i}] {doc.title} - {doc.content[:100]}...")
            if len(results) >= max_results:
                break
        return results or ["æœªæ‰¾åˆ°åŒ¹é…ç»“æœã€‚"]

    def delete_document_by_index(self, index: int) -> str:
        if 0 <= index < len(self.documents):
            del self.documents[index]
            # é‡æ–°æ„å»ºç´¢å¼•
            embeddings = np.array([doc.embedding for doc in self.documents if doc.embedding is not None])
            self.index = faiss.IndexFlatIP(self.embedding_dim)
            if len(embeddings) > 0:
                faiss.normalize_L2(embeddings)
                self.index.add(embeddings)
            self.save()
            return f"å·²åˆ é™¤ç¬¬ {index} ä¸ªæ–‡æ¡£ç‰‡æ®µå¹¶é‡æ–°æ„å»ºç´¢å¼•ã€‚"
        return f"æ— æ•ˆç´¢å¼•ï¼š{index}"


# -------------------- å›å¤ç”Ÿæˆæ¨¡å— --------------------

class AnswerGenerator:
    def __init__(self, api_key: str):
        self.client = OpenAI(api_key=api_key, base_url="https://api.deepseek.com")

    def generate_base_answer(self, query: str) -> str:
        # å¤§æ¨¡å‹ç›´æ¥æ ¹æ®æé—®å›ç­”ï¼Œæ— ä¸Šä¸‹æ–‡
        response = self.client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸­æ–‡é—®ç­”åŠ©æ‰‹"},
                {"role": "user", "content": query}
            ],
            temperature=0.7
        )
        return response.choices[0].message.content.strip()

    def generate_enhanced_answer(self, query: str, docs: List[Document], base_answer: str) -> str:
        if not docs:
            # æ²¡æœ‰æ£€ç´¢åˆ°ç›¸å…³æ–‡æ¡£ï¼Œç›´æ¥è¿”å›åŸºç¡€å›ç­”
            return base_answer

        context = "\n".join([f"[{i+1}] {doc.content}" for i, doc in enumerate(docs)])
        prompt = f"""ä½ æ˜¯ä¸“ä¸šçš„ä¸­æ–‡é—®ç­”åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹æ–‡æ¡£å†…å®¹å’Œå¤§æ¨¡å‹åŸå›ç­”ï¼Œç»“åˆè‡ªèº«çŸ¥è¯†ï¼Œç”Ÿæˆä¸€ä¸ªæ›´å…¨é¢ã€è¯¦ç»†ã€æœ‰æ¡ç†çš„è§£ç­”ï¼Œå¹¶åœ¨ä½¿ç”¨æ–‡æ¡£ä¿¡æ¯æ—¶æ³¨æ˜æ¥æºç¼–å·ï¼ˆå¦‚ [1]ã€[2]ï¼‰ã€‚

è¦æ±‚ï¼š
- ç”¨è¾ƒé•¿æ®µè½ï¼Œé€ç‚¹å±•å¼€è¯´æ˜ï¼›
- ä¼˜å…ˆä½¿ç”¨æ–‡æ¡£ä¿¡æ¯å›ç­”ï¼Œå¿…è¦æ—¶è¡¥å……é€šè¯†ï¼›
- ä¸è¦çœç•¥ç»†èŠ‚ï¼›
- ç»“æ„æ¸…æ™°ã€é€»è¾‘ä¸¥è°¨ï¼›
- ä¿æŒä¸­æ–‡ä¹¦é¢è¯­é£æ ¼ã€‚

æ–‡æ¡£å†…å®¹ï¼š
{context}

å¤§æ¨¡å‹åŸå›ç­”ï¼š
{base_answer}

é—®é¢˜ï¼š
{query}

è¯·å¼€å§‹ä½œç­”ï¼š
"""

        response = self.client.chat.completions.create(
            model="deepseek-chat",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸­æ–‡é—®ç­”åŠ©æ‰‹"},
                {"role": "user", "content": prompt}
            ],
            temperature=0.7
        )
        return response.choices[0].message.content.strip()


# -------------------- RAGç³»ç»Ÿ --------------------

class RAGSystem:
    def __init__(self, api_key: str):
        self.processor = DocumentProcessor()
        self.embedder = EmbeddingModel()
        self.vector_store = VectorStore(self.embedder.embedding_dim)
        self.generator = AnswerGenerator(api_key)

    def add_pdf(self, pdf_file) -> str:
        pdf = fitz.open(stream=pdf_file.read(), filetype="pdf")
        text = "\n".join([page.get_text() for page in pdf])
        docs = self.processor.process_document(text, title=pdf.name, source=pdf.name)
        embeddings = self.embedder.encode_texts([doc.content for doc in docs])
        self.vector_store.add_documents(docs, embeddings)
        return f"æˆåŠŸå¤„ç†æ–‡æ¡£ï¼Œå…±{len(docs)}ä¸ªç‰‡æ®µ"

    def query(self, question: str) -> QueryResult:
        start = time.time()
        q_embed = self.embedder.encode_query(question)
        top_docs = self.vector_store.search(q_embed)
        retrieval_time = time.time() - start

        # å…ˆè°ƒç”¨å¤§æ¨¡å‹åŸå›ç­”
        base_answer = self.generator.generate_base_answer(question)

        start = time.time()
        enhanced_answer = self.generator.generate_enhanced_answer(question, [doc for doc, _ in top_docs], base_answer)
        generation_time = time.time() - start

        avg_score = sum(score for _, score in top_docs) / (len(top_docs) or 1)

        return QueryResult(
            query=question,
            answer=enhanced_answer,
            source_documents=[doc for doc, _ in top_docs],
            confidence_score=avg_score,
            retrieval_time=retrieval_time,
            generation_time=generation_time
        )


# -------------------- Gradioç•Œé¢ --------------------

rag = RAGSystem(api_key=os.getenv("DEEPSEEK_API_KEY", "sk-de353379ea494b98b197dce3b8ce5391"))

def upload_pdf_in_blocks(pdf_file):
    if not pdf_file:
        return "è¯·ä¸Šä¼ æœ‰æ•ˆçš„PDFæ–‡ä»¶ã€‚"
    import fitz
    pdf = fitz.open(pdf_file.name)
    text = "\n".join([page.get_text() for page in pdf])
    docs = rag.processor.process_document(text, title=Path(pdf_file.name).name, source=Path(pdf_file.name).name)
    embeddings = rag.embedder.encode_texts([doc.content for doc in docs])
    rag.vector_store.add_documents(docs, embeddings)
    return f"æˆåŠŸå¤„ç†æ–‡æ¡£ï¼Œå…±{len(docs)}ä¸ªç‰‡æ®µ"

def qa_interface_in_blocks(question):
    result = rag.query(question)
    docs = "\n\n".join([f"[{i+1}] {doc.content[:200]}..." for i, doc in enumerate(result.source_documents)])
    return f"ã€å›å¤ã€‘{result.answer}\n\nã€æ¥æºæ–‡æ¡£ç‰‡æ®µã€‘\n{docs}\n\nç½®ä¿¡åº¦: {result.confidence_score:.2f}ï¼Œæ£€ç´¢: {result.retrieval_time:.2f}sï¼Œç”Ÿæˆ: {result.generation_time:.2f}s"

with gr.Blocks() as app:
    gr.Markdown("## åŸºäºRAGçš„è¡Œä¸šçŸ¥è¯†é—®ç­”ç³»ç»Ÿ")
    with gr.Row():
        with gr.Column(scale=1):
            pdf_input = gr.File(label="ä¸Šä¼ PDFæ–‡æ¡£", file_types=['.pdf'])
            upload_feedback = gr.Textbox(label="ä¸Šä¼ åé¦ˆ")
            upload_btn = gr.Button("ä¸Šä¼ æ–‡æ¡£")
        with gr.Column(scale=2):
            question_input = gr.Textbox(label="è¯·è¾“å…¥ä½ çš„é—®é¢˜")
            answer_output = gr.Textbox(label="å›ç­”ç»“æœ")
            ask_btn = gr.Button("æé—®")
    with gr.Row():
        with gr.Column():
            gr.Markdown("### å‘é‡æ•°æ®åº“ç®¡ç†")
            list_btn = gr.Button("ğŸ“‹ æŸ¥çœ‹æ‰€æœ‰æ–‡æ¡£ç‰‡æ®µ")
            list_output = gr.Textbox(label="æ–‡æ¡£åˆ—è¡¨", lines=10)

            search_input = gr.Textbox(label="å…³é”®è¯æœç´¢")
            search_btn = gr.Button("ğŸ” æœç´¢")
            search_output = gr.Textbox(label="æœç´¢ç»“æœ", lines=5)

            delete_input = gr.Number(label="åˆ é™¤æ–‡æ¡£ç‰‡æ®µç¼–å·")
            delete_btn = gr.Button("âŒ åˆ é™¤")
            delete_output = gr.Textbox(label="åˆ é™¤ç»“æœ")

    upload_btn.click(upload_pdf_in_blocks, inputs=pdf_input, outputs=upload_feedback)
    ask_btn.click(qa_interface_in_blocks, inputs=question_input, outputs=answer_output)
    list_btn.click(fn=lambda: "\n".join(rag.vector_store.list_documents()),
                   outputs=list_output)

    search_btn.click(fn=lambda kw: "\n".join(rag.vector_store.search_by_keyword(kw)),
                     inputs=search_input,
                     outputs=search_output)

    delete_btn.click(fn=lambda i: rag.vector_store.delete_document_by_index(int(i)),
                     inputs=delete_input,
                     outputs=delete_output)


if __name__ == "__main__":
    app.launch()
